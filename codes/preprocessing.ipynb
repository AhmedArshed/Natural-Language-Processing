{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy love film youngest scared captain hook story great think little boy really connect beautiful fun music\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "a = stop_words.ENGLISH_STOP_WORDS\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "from nltk import ngrams\n",
    "from string import punctuation as punc\n",
    "\n",
    "review_col =[]\n",
    "def opperations():\n",
    "    corpus = open('Movies_TV.txt').read()\n",
    "    corpus = re.sub(r'Domain.*\\n', '', corpus)\n",
    "    rows = corpus.split('\\n')\n",
    "    rows.remove(rows[-1])\n",
    "\n",
    "    domain_col = []\n",
    "    lable_col = []\n",
    "    rating = []\n",
    "\n",
    "    for r in rows:   ## sepreating colomes\n",
    "        dom, label, rate, review = r.split('\\t')\n",
    "        domain_col.append(dom)\n",
    "        lable_col.append(label)\n",
    "        rating.append(rate)\n",
    "        review_col.append(review)\n",
    "    \n",
    "    first_list = []\n",
    "    for w in review_col:   ##striping and lower the letters\n",
    "        rew = ''.join(w)\n",
    "        rew = rew.strip()\n",
    "        rew = rew.lower()\n",
    "        first_list.append(rew)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    second_list = []\n",
    "    threed_list = []\n",
    "    str2 =''\n",
    "\n",
    "    for i in range(len(first_list)):\n",
    "        str2 = ''\n",
    "        s = listToString(first_list[i])\n",
    "        for w in s:\n",
    "            if w not in punc: ##removing punctuation\n",
    "                str2 += w\n",
    "        second_list.append(str2)\n",
    "\n",
    "          \n",
    "    for i in range(len(second_list)):\n",
    "        string = ''\n",
    "        st = second_list[i].split() ## removing stop words\n",
    "        for j in st:\n",
    "            if j not in a:\n",
    "                string += j\n",
    "                string += \" \"\n",
    "        threed_list.append(string)\n",
    "\n",
    "    for i in range(len(threed_list)):\n",
    "        filtered_sentence.append(re.sub(' +', ' ', threed_list[i].rstrip()))  ## removing white spaces\n",
    "#     print(filtered_sentence[0])\n",
    "    \n",
    "    \n",
    "    word_formed = []\n",
    "    word_formed2 = []\n",
    "    str3 = ''\n",
    "    filtered_sentence = removing_space(filtered_sentence)\n",
    "    for i in range(len(filtered_sentence)):\n",
    "        str3 =''\n",
    "        st = filtered_sentence[i].split()\n",
    "        for w in st:\n",
    "            str3 += ps.stem(w)\n",
    "            str3 += \" \"\n",
    "        word_formed.append(str3)\n",
    "        word_formed = removing_space(word_formed)\n",
    "        \n",
    "        \n",
    "    for i in range(len(filtered_sentence)):\n",
    "        str4 =''\n",
    "        st = filtered_sentence[i].split()\n",
    "        for w in st:\n",
    "            str4 += wl.lemmatize(w)\n",
    "            str4 += \" \"\n",
    "        word_formed2.append(str4)\n",
    "        word_formed2 = removing_space(word_formed2)\n",
    "    return word_formed\n",
    "    \n",
    "    \n",
    "def removing_space(a):\n",
    "    filtered_sentence = []\n",
    "    for i in range(len(a)):\n",
    "        filtered_sentence.append(a[i].rstrip())\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \"\"  \n",
    "    for ele in s:  \n",
    "        str1 += ele     \n",
    "    return str1\n",
    "\n",
    "\n",
    "def grams():\n",
    "    word_formed = opperations()\n",
    "    first_review = []\n",
    "    first_review = word_formed[0]\n",
    "    a = first_review.split()\n",
    "    unigrams = list(ngrams(a, 1)) ## getting unigrams\n",
    "    bigrams = list(ngrams(a, 2))  ## getting bigrams\n",
    "    trigrams = list(ngrams(a, 3)) ## getting trigrams\n",
    "    print(\"unigrams\", unigrams)\n",
    "    print(\"             \")\n",
    "    print(\"bigrams\", bigrams)\n",
    "    print(\"             \")\n",
    "    print(\"trigrams\", trigrams)\n",
    "    \n",
    "    unigrams_freq = [unigrams.count(x)/len(set(unigrams)) for x in unigrams]\n",
    "    print(unigrams_freq)\n",
    "    \n",
    "    bigrams_freq = [bigrams.count(x)/unigrams.count(x[:1]) for x in bigrams]\n",
    "    print(bigrams_freq)  ## probily of bigrams\n",
    "    \n",
    "    trigrams_freq = [trigrams.count(x)/bigrams.count(x[:2]) for x in trigrams]\n",
    "    print(trigrams_freq) ## probily of trigrams\n",
    "\n",
    "def toking():    \n",
    "    word_formed = opperations()\n",
    "    a = 0\n",
    "    for w in word_formed:\n",
    "        a += len(w.split(\" \"))\n",
    "    print(a)\n",
    "    \n",
    "def after_vocabulary():\n",
    "    word_formed = opperations()\n",
    "    a = 0\n",
    "    check_lst = []\n",
    "    for w in word_formed:\n",
    "        lis = w.split()\n",
    "        for i in lis:\n",
    "            check_lst.append(i)\n",
    "#     print(check_lst)\n",
    "    \n",
    "    a = len(set(check_lst))\n",
    "    \n",
    "    print(\"vocabulary after preprocessing\" , a)\n",
    "    \n",
    "def before_vocabulary():\n",
    "    a = 0\n",
    "    check_lst = []\n",
    "    for w in review_col:\n",
    "        lis = w.split()\n",
    "        for i in lis:\n",
    "            check_lst.append(i)\n",
    "#     print(check_lst)\n",
    "    \n",
    "    v = len(set(check_lst))\n",
    "    \n",
    "    print(\"vocabulary before preprocessing\" , v)\n",
    "    \n",
    "def average_lenght():\n",
    "    a = 0\n",
    "    word_formed = opperations()\n",
    "    total_reviews = 0\n",
    "    for w in word_formed:\n",
    "        total_reviews += 1\n",
    "#     total_reviews\n",
    "    for w in word_formed:\n",
    "        for i in w:\n",
    "            a += 1\n",
    "    print(a/total_reviews)\n",
    "    \n",
    "def average_lenght_token():\n",
    "    word_formed = opperations()\n",
    "    total_reviews = 0\n",
    "    for w in word_formed:\n",
    "        total_reviews += 1\n",
    "    \n",
    "    check_lst = []\n",
    "    \n",
    "    a = 0\n",
    "    for w in word_formed:\n",
    "        lis = w.split(\" \")\n",
    "        a += len(lis)\n",
    "    print(a)\n",
    "    print(a/total_reviews)\n",
    "    \n",
    "grams()\n",
    "toking()\n",
    "after_vocabulary()\n",
    "before_vocabulary()\n",
    "average_lenght()\n",
    "average_lenght_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy love film youngest scare captain hook stori great think littl boy realli connect beauti fun music'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# ps = PorterStemmer()   ##stemming word\n",
    "\n",
    "# word_formed = []\n",
    "# word_formed2 = []\n",
    "# str3 = ''\n",
    "# filtered_sentence = removing_space(threed_list)\n",
    "# for i in range(len(filtered_sentence)):\n",
    "#     str3 =''\n",
    "#     st = filtered_sentence[i].split()\n",
    "#     for w in st:\n",
    "#         str3 += ps.stem(w)\n",
    "#         str3 += \" \"\n",
    "#     word_formed.append(str3)\n",
    "#     word_formed = removing_space(word_formed)\n",
    "# word_formed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy love film youngest scared captain hook story great think little boy really connect beautiful fun music'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wl = WordNetLemmatizer()\n",
    "# word_formed = []\n",
    "# str3 = ''\n",
    "# for i in range(len(filtered_sentence)):\n",
    "#     str3 =''\n",
    "#     st = filtered_sentence[i].split()\n",
    "#     for w in st:\n",
    "#         str3 += wl.lemmatize(w)\n",
    "#         str3 += \" \"\n",
    "#     word_formed.append(str3)\n",
    "#     word_formed = removing_space(word_formed)\n",
    "# word_formed[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams [('boy',), ('love',), ('film',), ('youngest',), ('scared',), ('captain',), ('hook',), ('story',), ('great',), ('think',), ('little',), ('boy',), ('really',), ('connect',), ('beautiful',), ('fun',), ('music',)]\n",
      "             \n",
      "bigrams [('boy', 'love'), ('love', 'film'), ('film', 'youngest'), ('youngest', 'scared'), ('scared', 'captain'), ('captain', 'hook'), ('hook', 'story'), ('story', 'great'), ('great', 'think'), ('think', 'little'), ('little', 'boy'), ('boy', 'really'), ('really', 'connect'), ('connect', 'beautiful'), ('beautiful', 'fun'), ('fun', 'music')]\n",
      "             \n",
      "trigrams [('boy', 'love', 'film'), ('love', 'film', 'youngest'), ('film', 'youngest', 'scared'), ('youngest', 'scared', 'captain'), ('scared', 'captain', 'hook'), ('captain', 'hook', 'story'), ('hook', 'story', 'great'), ('story', 'great', 'think'), ('great', 'think', 'little'), ('think', 'little', 'boy'), ('little', 'boy', 'really'), ('boy', 'really', 'connect'), ('really', 'connect', 'beautiful'), ('connect', 'beautiful', 'fun'), ('beautiful', 'fun', 'music')]\n"
     ]
    }
   ],
   "source": [
    "# from nltk import ngrams\n",
    "# first_review = []\n",
    "# first_review = word_formed[0]\n",
    "# a = first_review.split()\n",
    "# unigrams = list(ngrams(a, 1))\n",
    "# bigrams = list(ngrams(a, 2))\n",
    "# trigrams = list(ngrams(a, 3))\n",
    "# print(\"unigrams\", unigrams)\n",
    "# print(\"             \")\n",
    "# print(\"bigrams\", bigrams)\n",
    "# print(\"             \")\n",
    "# print(\"trigrams\", trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.125,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.125,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 0.0625]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigrams_freq = [unigrams.count(x)/len(set(unigrams)) for x in unigrams]\n",
    "# unigrams_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams_freq = [bigrams.count(x)/unigrams.count(x[:1]) for x in bigrams]\n",
    "# bigrams_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigrams_freq = [trigrams.count(x)/bigrams.count(x[:2]) for x in trigrams]\n",
    "# trigrams_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
